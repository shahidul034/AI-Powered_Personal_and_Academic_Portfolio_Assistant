# Dataset for Classifying Human vs. AI-Generated Answers in Applied Statistics

---

## Abstract
This paper introduces a new dataset designed to help classify answers as either human-generated or AI-generated, specifically within the domain of **Applied Statistics**. The dataset was created to address the challenge faced by instructors in identifying the authenticity of student assignments due to the widespread use of Large Language Models (LLMs) like ChatGPT. The dataset contains 4,231 question-and-answer pairs for 116 unique questions, with answers collected from 100 undergraduate students and an equivalent number of answers generated by ChatGPT. The researchers also developed and tested a data collection framework that can be adapted for creating similar datasets in other subject areas.

---

## Problem Statement
The increasing use of **Large Language Models (LLMs)** like ChatGPT by students to complete assignments poses a significant challenge for course instructors trying to verify the originality of submitted work. While general-purpose AI detection tools exist, they often perform poorly on domain-specific texts, such as those related to medicine or coding. There is a need for a specialized dataset to train and benchmark AI detection tools tailored for the Applied Statistics domain.

---

## Objectives
- To create a comprehensive dataset for classifying human- and AI-generated answers in the field of Applied Statistics.
- To develop a data collection framework that can be used to generate similar, domain-specific datasets for other subjects.
- To demonstrate the utility of the dataset by training and testing several transformer-based classification models and evaluating their performance.

---

## Methodology
The researchers followed a four-step framework to create the dataset:

1.  **Question Selection**: Domain experts with PhDs and over five years of teaching experience in Applied Statistics selected 116 questions covering a wide range of topics.
2.  **Student and Answer Collection**: One hundred undergraduate students from a single university volunteered to participate. Each student was randomly assigned 50 questions to answer manually and was asked to generate an AI answer for the same questions using ChatGPT. A custom-built web platform was used to distribute questions and collect answers.
3.  **Data Curation**: The collected human answers were checked for plagiarism using Turnitin software, and any plagiarized or inappropriate answers were removed to ensure the originality and quality of the dataset.
4.  **Data Formatting**: The final dataset was compiled into two main formats: 100 separate Excel files (one for each student) and a single, unified JSON list file named `dataset.jsonl` for ease of use. The JSON file contains four attributes for each entry: an ID, the original question, the answer text, and a binary flag (`Is_it_AI`) to indicate whether the answer was generated by a human (0) or an AI (1).

---

## Key Findings
- The final dataset contains a total of **4,231 question-and-answer combinations**.
- AI-generated answers were found to be significantly longer than human-generated answers, with an average length of **151.2 words** compared to **77.3 words** for human answers.
- Human responses showed greater vocabulary diversity, with **11,801 unique words** compared to **10,318** in the AI-generated responses.
- When tested with the dataset, several transformer models achieved a high level of accuracy in classifying answers. For example, the `DistilBERT base uncased finetuned SST-2` model achieved **92% accuracy**, while others like `Bert-base-uncased` and `Albert-base-v2` also showed strong performance at **89%** and **86%** accuracy, respectively. This demonstrates the dataset's effectiveness for training and benchmarking AI detection models.
- The number of answers received for each of the 116 questions ranged from 34 to 40. 
---

## Contributions
- **A new, publicly available dataset** specifically for AI vs. human text classification in the Applied Statistics domain.
- **A reusable data collection framework** that can be easily adapted to create similar subject-specific datasets, addressing the limitations of generic AI detection tools.
- **Empirical evidence** that AI-generated answers in a specific domain can be distinguished from human-generated answers based on measurable characteristics like length and vocabulary diversity.

---

## Applications
- **Developing and training AI detection tools** for academic assignments in Applied Statistics.
- **Benchmarking the performance** of existing and new AI-detector tools.
- **Facilitating research** into the linguistic patterns and characteristics of AI-generated text compared to human-generated text.
- The data collection framework can be used by researchers and educators to gather data for other domain-specific AI detection challenges.

---

## Limitations
- The dataset's diversity is limited because all participating students were from a single university with a similar educational background.
- All AI-generated answers were sourced exclusively from a single LLM, ChatGPT, which may not represent the output of other AI models.

---

## Future Directions
- Involving students from multiple universities to increase the diversity of the human-generated data.
- Including answers from different AI tools besides ChatGPT to create a more comprehensive and varied dataset.
- Extending the data collection framework to other academic disciplines beyond Applied Statistics.

---

## Keywords
- Large Language Models (LLM)
- AI
- Applied Statistics
- Dataset
- Transformer Models
- Natural Language Processing
- Human vs. AI Text Classification