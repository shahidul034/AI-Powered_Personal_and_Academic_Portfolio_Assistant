# Fake News Detection Using Ensemble and Deep Learning Models

---

## Abstract
This study investigates the effectiveness of various machine learning and deep learning models for detecting fake news. The research employed a benchmark dataset and tested a wide range of classifiers, including traditional models, as well as ensemble and transformer-based architectures. The input text was processed using two vectorization techniques: **Tf-idf** and **Count vectorization**. To enhance performance, the top-performing models were combined using **soft and hard voting ensembles**. The **BERT model**, optimized with the Adam optimizer, achieved the highest overall accuracy of **99.93%**, with perfect precision and high recall. Among the ensemble models, a soft voting ensemble of the top five classifiers (LightGBM, XGBoost, Random Forest, Decision Tree, and Logistic Regression) with Count vectorization achieved the best performance with an accuracy of **99.87%**.

---

## Problem Statement
In the modern information ecosystem, characterized by rapid data dissemination and digital interconnectedness, the spread of fake news poses a significant threat to societal stability and individual decision-making. The authenticity of news consumed online is often questionable, as information is frequently shared without verification. The proliferation of rumors on social media platforms can trigger widespread turmoil and have serious real-world consequences, such as inciting panic and causing accidents. There is a critical need for effective and efficient systems that can automatically detect and distinguish between real and fake news in large-scale datasets.

---

## Objectives
- To explore the efficiency of various classifier models for fake news detection on an open-source dataset.
- To compare the performance of traditional machine learning, deep learning, and transformer-based models.
- To investigate the effectiveness of soft and hard voting ensembles for boosting the performance of the top-performing models.
- To determine the optimal text vectorization method for improving classification accuracy.

---

## Methodology
The study used a multi-stage approach to build and evaluate models for fake news detection.

1.  **Dataset**: The well-known ISOT benchmark dataset was used, which consists of two CSV files: `True.csv` and `Fake.csv`. The dataset has a balanced distribution with 23,481 fake news articles and 21,417 real news articles.

2.  **Preprocessing**: The `True.csv` and `Fake.csv` files were first concatenated, with each record assigned a class label (1 for true, 0 for fake). The text data was then cleaned by converting it to lowercase and removing special characters, URLs, HTML tags, punctuation, and numbers using regular expression substitution operations.

3.  **Feature Vector Generation**: Three vectorization techniques were used to convert the cleaned text into numerical feature vectors:
    - **Tf-idf (Term frequency-inverse document frequency) Vectorizer**: This technique calculates the importance of words by considering their frequency within a document and their rarity across the entire corpus.
    - **Count Vectorizer**: This technique simply counts the frequency of words (bi-grams in this study) to create a matrix of token counts. The input sequence length was set to 150, and zero postfix padding was applied to ensure uniform vector length.
    - **BERT Tokenizer**: This tokenizer converts raw text into numeric tokens, which are then used as input for the BERT model's embeddings.

4.  **Model Training**: The preprocessed dataset was split into training and testing sets with a 75-25 ratio. The study evaluated three types of models:
    - **Classifier Models**: K-Nearest Neighbors (KNN), Logistic Regression (LR), Decision Tree (DT), LightGBM, XGBoost, Gradient Boosting (GB), and Naive Bayes (NB).
    - **Ensemble Models**: Soft and hard voting ensembles were applied to the top three and top five performing classifiers identified from the initial tests.
    - **Deep Learning Models**: Convolutional Neural Network (CNN) and BERT models.

5.  **Optimization**: Different optimizers (Adam, RMSprop, SGD, Adadelta, and Adagrad) were tested to find the best configuration for the deep learning models.

---

## Key Findings
- **BERT Achieved Highest Accuracy**: The BERT model, optimized with the Adam optimizer, achieved the highest performance across all metrics, with an accuracy of **99.93%**, a precision of **100%**, and a recall of **99.84%**.
- **Ensemble Models Performed Best Among Traditional Classifiers**: A soft voting ensemble of the top five models (LightGBM, XGBoost, Random Forest, Decision Tree, and Logistic Regression) achieved the highest performance among non-deep learning models, with a **99.87%** accuracy when used with Count vectorization.
- **Count Vectorization Outperformed Tf-idf**: The Count vectorization method consistently produced better results than Tf-idf for most classifier models, suggesting its effectiveness in capturing important language nuances for this task.
- **Optimizer Choice is Critical**: For deep learning models like BERT and CNN, the choice of optimizer was crucial. The **Adam optimizer** consistently led to superior performance and stability, while others like RMSprop and SGD resulted in significantly lower accuracy.
- **Ensemble Voting is Stable and Efficient**: The performance of soft and hard voting ensembles remained consistent, with only negligible differences in accuracy. Using just the top three models maintained impressive results without significant performance loss.

---

## Contributions
- The study provides an extensive performance analysis of various classifier models, ensemble techniques, and optimizers for fake news detection.
- It highlights the efficiency and applicability of using both soft and hard voting ensembles on top-performing models, a methodology not extensively explored in previous studies.
- The research confirms that the Adam optimizer is the most effective for training deep learning models like BERT and CNN for fake news detection tasks.

---

## Applications
- The models and methodologies can be applied to real-world applications in **news verification** and **social media monitoring**.
- The findings can help media organizations and social platforms streamline their efforts to identify fake news in large datasets.

---

## Limitations
The paper does not explicitly state any limitations or weaknesses of the proposed method.

---

## Future Directions
The paper does not explicitly discuss future research directions.

---

## Keywords
- Fake news detection
- Classifier models
- Ensemble model
- BERT model
- CNN model
- Count vectorization
- Tf-idf
- Machine learning
- Deep learning
- Social media